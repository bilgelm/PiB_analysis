{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PiB Data : 1st and 2nd order models [WIP]\n",
    "\n",
    "* using [GP](https://gpytorch.ai/) (differentiable with PyTorch=1.3), **can improve initialization of loglikelihood object**\n",
    "* Draft for *quick* experiments prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import sys\n",
    "\n",
    "# Setting paths to directory roots | >> hgn_reproduce\n",
    "parent = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
    "print(parent)\n",
    "sys.path.insert(0, parent)\n",
    "os.chdir(parent)\n",
    "print('Setting root path to : {}'.format(parent))\n",
    "\n",
    "# Generic\n",
    "import math\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.stats import ranksums\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# utils\n",
    "import gpytorch\n",
    "import torchdiffeq\n",
    "from src.utils.op import gpu_numpy_detach\n",
    "from src.utils.datasets import get_fromdataframe\n",
    "from src.utils.models import ExactGPModel, Christoffel, Hamiltonian\n",
    "from src.utils.integrators import torchdiffeq_torch_integrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables + read data and extract corresponding dataframe, save it if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Users/paul.vernhet/PhD_Aramis/experiments/data/ode_data/wrap_summer2019'\n",
    "min_visits = 3\n",
    "pib_threshold = 1.2\n",
    "var_data = 0.07**2\n",
    "seed = 666\n",
    "batch_size = 16\n",
    "standardize = True\n",
    "\n",
    "def get_dataframe(data_path, min_visits):\n",
    "    \"\"\"\n",
    "    :param data_path: absolute path to data\n",
    "    :return: dataframe with columns (time (list) | values (list) | apoe_all1 (int) | apoe_all2 (int))\n",
    "    \"\"\"\n",
    "    # Get data (as pandas dataframes)\n",
    "    df_pib = pd.read_sas(os.path.join(data_path, 'pib.sas7bdat'), format='sas7bdat')\n",
    "    # df_mri = pd.read_sas(os.path.join(data_path, 'mri.sas7bdat'), format='sas7bdat')\n",
    "    df_demo = pd.read_sas(os.path.join(data_path, 'demo.sas7bdat'), format='sas7bdat')\n",
    "    # df_pacc = pd.read_sas(os.path.join(data_path, 'pacc.sas7bdat'), format='sas7bdat')\n",
    "\n",
    "    # Preprocess data\n",
    "    df_demo['reggieid'] = df_demo.reggieid.astype(int)\n",
    "    df_pib['reggieid'] = df_pib.reggieid.astype(int)\n",
    "    df_pib.head()\n",
    "    df_time = df_pib.groupby(['reggieid'])['pib_age'].apply(list)\n",
    "    df_values = df_pib.groupby(['reggieid'])['pib_index'].apply(list)\n",
    "    df_merged = pd.concat([df_time, df_values], axis=1)\n",
    "    assert len(df_time) == len(df_values) == len(df_merged)\n",
    "    print('Number of patients : {:d}'.format(len(df_merged)))\n",
    "    df_merged = df_merged[df_merged['pib_age'].apply(lambda x: len(x)) >= min_visits]\n",
    "    print('Number of patients with visits > {} time : {:d}'.format(min_visits - 1, len(df_merged)))\n",
    "\n",
    "    # Final merge\n",
    "    df = df_merged.join(df_demo.set_index('reggieid')[['apoe_all1', 'apoe_all2']])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data, viz training sequences :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- DATA GENERATION\n",
    "df = get_dataframe(data_path=data_dir, min_visits=min_visits)\n",
    "train_loader, val_loader, test_loader, all_loader, (times_mean, times_std), (data_mean, data_std) = get_fromdataframe(df=df, batch_size=batch_size, \n",
    "                                                                                                                      standardize=standardize, seed=seed)\n",
    "destandardize_time = lambda time: time * times_std + times_mean if standardize else time\n",
    "destandardize_data = lambda data: data * data_std + data_mean if standardize else data\n",
    "destandardize_d1 = lambda der: der * data_std / times_std if standardize else data\n",
    "totorch = lambda data, form: torch.from_numpy(data).float().view(-1, 1) if form == 'float' else torch.from_numpy(data).double().view(-1, 1)\n",
    "\n",
    "# -------- VIZ\n",
    "fig, ax = plt.subplots(3, 1, figsize=(15, 15))\n",
    "for i, (data_loader, name) in enumerate(zip([train_loader, val_loader, test_loader], ['training', 'validation', 'test'])):\n",
    "    ax[i].set_title('{} set'.format(name))\n",
    "    ax[i].set_xlabel('Age')\n",
    "    ax[i].set_ylabel('PiB Score')\n",
    "    for batch_idx, (positions, maskers, times, sequences, _) in enumerate(data_loader):\n",
    "        for position, masker, time, sequence in zip(positions, maskers, times, sequences):\n",
    "            t = gpu_numpy_detach(destandardize_time(time[masker == True]))\n",
    "            s = gpu_numpy_detach(destandardize_data(sequence[masker == True]))\n",
    "            ax[i].plot(t, s, '--o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Quadratic regression + polynomial regression for second order with Christoffel symbols : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ PERFORM REGRESSION\n",
    "polynomial_features = PolynomialFeatures(degree=2,  include_bias=True)\n",
    "alpha_ridge = 1e-10 if standardize else 1e1\n",
    "ridge_regression = Ridge(alpha=alpha_ridge, fit_intercept=False)\n",
    "which_regression = 'ridge'\n",
    "assert which_regression in ['ridge', 'sigmoid']\n",
    "x_real = []\n",
    "t_real = []\n",
    "stats_derivatives = {'t_bar':[], 'means':[], 'bias':[], 'covars':[]}\n",
    "\n",
    "\n",
    "for batch_idx, (positions, maskers, times, sequences, _) in enumerate(all_loader):\n",
    "    for position, masker, time, sequence in zip(positions, maskers, times, sequences):\n",
    "        \n",
    "        # ------ Read STANDARDIZED data (as Ridge regression is not scale invariant)\n",
    "        t = gpu_numpy_detach(time[masker == True])\n",
    "        s = gpu_numpy_detach(sequence[masker == True])\n",
    "        n = len(t)\n",
    "        assert n > 2, \"At least 3 points required for a 2nd order derivative estimate\"\n",
    "        t_bar = np.mean(t)\n",
    "        \n",
    "        if which_regression == 'ridge':\n",
    "            # ------ Ridge Regression\n",
    "            # i) Fit Ridge regression\n",
    "            t_poly = polynomial_features.fit_transform(t.reshape(-1, 1))\n",
    "            ridge_regression.fit(t_poly, s)\n",
    "            theta = np.array(ridge_regression.coef_)\n",
    "            A_func = lambda t_ref: np.array([[1, t_ref, t_ref**2], [0., 1., 2*t_ref], [0., 0., 2]])\n",
    "            A_bar = A_func(t_bar)\n",
    "\n",
    "            # ii) Regress fitted data at mean time point (ie t_bar)\n",
    "            s_hat = A_bar.dot(theta)\n",
    "\n",
    "            # iii) Store bias and variances on (biased) estimator\n",
    "            H = np.linalg.inv(np.transpose(t_poly).dot(t_poly) + alpha_ridge*n*np.eye(3)).dot(np.transpose(t_poly).dot(t_poly))\n",
    "            bias_theta = H.dot(theta)\n",
    "            covar_theta = var_data*H.dot(np.linalg.inv(np.transpose(t_poly).dot(t_poly) + alpha_ridge*n*np.eye(3)))\n",
    "            bias_derivatives = A_bar.dot(bias_theta)\n",
    "            covars_derivatives = A_bar.dot(covar_theta).dot(np.transpose(A_bar))   \n",
    "        else:\n",
    "            # ------ Sigmoid Regression under isotonic constraint (c > 0)\n",
    "            # i) Fit logistic regression by OLS\n",
    "            def np_sigmoid(x, *p):\n",
    "                x0, y0, c = p\n",
    "                return c / (1 + np.exp(-(x-x0))) + y0\n",
    "            \n",
    "            def np_velocity_sigmoid(x, *p):\n",
    "                x0, y0, c = p\n",
    "                sigma = np_sigmoid(x, *p)\n",
    "                return 1. / c * (sigma - y0) * (c + y0 - sigma)\n",
    "            \n",
    "            def np_acceleration_sigmoid(x, *p):\n",
    "                x0, y0, c = p\n",
    "                sigma = np_sigmoid(x, *p)\n",
    "                return (1. / c) ** 2 * (sigma - y0) * (c + y0 - sigma) * (c + 2 * y0 - 2 * sigma)\n",
    "            \n",
    "            p_guess = np.array([np.median(t), np.median(s), 1.0])\n",
    "            eps = 1e-3\n",
    "            p_star, p_cov = curve_fit(f=np_sigmoid, xdata=t, ydata=s, p0=p_guess, bounds=([np.min(t), np.min(s), eps], [np.max(t), np.max(s), np.inf] ))\n",
    "            \n",
    "            s_0 = np_sigmoid(t_bar, *p_star)\n",
    "            s_1 = np_velocity_sigmoid(t_bar, *p_star)\n",
    "            s_2 = np_acceleration_sigmoid(t_bar, *p_star)\n",
    "\n",
    "            # ii) Regress fitted data at mean time point (ie t_bar) --> second derivative follows sigmoid relationship\n",
    "            s_hat = np.array([s_0, s_1, s_2])\n",
    "\n",
    "            # iii) Bias and variances taken to be None here\n",
    "            bias_derivatives = None\n",
    "            covars_derivatives = None\n",
    "            \n",
    "        t_real.append(t)\n",
    "        x_real.append(s)\n",
    "        stats_derivatives['t_bar'].append(t_bar)\n",
    "        stats_derivatives['means'].append(s_hat)\n",
    "        stats_derivatives['bias'].append(bias_derivatives)\n",
    "        stats_derivatives['covars'].append(covars_derivatives)\n",
    "\n",
    "x_derivatives = np.transpose(np.stack(stats_derivatives['means']))\n",
    "\n",
    "# ------ Plot regressions\n",
    "\n",
    "def ax_ellipses(x_pos, y_pos, covar, ax, only_diag=False, scale=1., facecolor='none', **kwargs):\n",
    "    if only_diag:\n",
    "        eigenval = np.diagonal(covar)\n",
    "        eigenvec = np.eye(covar.shape[0])\n",
    "    else:\n",
    "        eigenval, eigenvec = np.linalg.eigh(covar)\n",
    "    ellipse = Ellipse((0, 0),\n",
    "        width=eigenval[0],\n",
    "        height=eigenval[1],\n",
    "        facecolor=facecolor,\n",
    "        **kwargs)\n",
    "    transf = transforms.Affine2D().rotate_deg(180. / np.pi * np.arctan(eigenvec[0, 1] / eigenvec[0, 0])).scale(scale, scale).translate(x_pos, y_pos)\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)\n",
    "\n",
    "# Gather informative data | filter out outliers\n",
    "u = x_derivatives[0]\n",
    "v = x_derivatives[1]\n",
    "v_squ = x_derivatives[1]**2\n",
    "w = x_derivatives[2]\n",
    "ratio = -1. * w / v_squ        # directly proportional to Christoffel symbol\n",
    "filter_quantiles = True\n",
    "eps_filter = .15\n",
    "if filter_quantiles:\n",
    "    filtered_index = np.where((ratio > np.quantile(ratio, eps_filter)) & (ratio < np.quantile(ratio, 1 - eps_filter)))\n",
    "    filtered_u = u[filtered_index]\n",
    "    filtered_ratio = ratio[filtered_index]\n",
    "    print('---\\nOriginal nb points : {} \\nRetained nb points : {}\\n---\\n'.format(len(ratio), len(filtered_ratio)))\n",
    "else:\n",
    "    filtered_index = np.arange(len(ratio))\n",
    "    filtered_u = u\n",
    "    filtered_ratio = ratio\n",
    "    print('---\\nNo filtering of extremal values\\n---\\n')\n",
    "\n",
    "# Plot figures\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "ax1.scatter(x=u, y=v, marker='o', color='k', s=10.) \n",
    "#for ui, vi, covi in zip(u, v, stats_derivatives['covars']):\n",
    "#    ax_ellipses(ui, vi, covi[:2,:2], ax=ax1, only_diag=True, scale=1., facecolor='green', edgecolor='red', alpha=0.3)\n",
    "ax1.set_xlim(np.min(u) - 1e-4, np.max(u) + 1e-4)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('x_dot')\n",
    "ax1.set_title('Estimates for ODE')\n",
    "\n",
    "ax2.scatter(x=v_squ, y=w, marker='o', color='k', s=10.)\n",
    "#for ui, vi, covi in zip(v_squ, w, stats_derivatives['covars']):\n",
    "#    ax_ellipses(ui, vi, covi[1:,1:], ax=ax2, only_diag=True, scale=1., facecolor='green', edgecolor='red', alpha=0.3)\n",
    "ax2.set_xlim(np.min(v_squ) - 1e-4, np.max(v_squ) + 1e-4)\n",
    "ax2.set_ylim(np.min(w) - 1e-4, np.max(w) + 1e-4)\n",
    "ax2.set_xlabel('x_dot^2')\n",
    "ax2.set_ylabel('x_dotdot')\n",
    "ax2.set_title('Estimates for derivate relationship (unused)')\n",
    "\n",
    "ax3.scatter(x=filtered_u, y=filtered_ratio, marker='o', color='k', s=10.)\n",
    "ax3.set_xlim(np.min(u) - 1e-4, np.max(u) + 1e-4)\n",
    "# ax3.set_ylim(np.min(ratio) - 1e-4, np.max(ratio) + 1e-4)\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('- x_dotdot / x_dot^2')\n",
    "ax3.set_title('Estimates for Christoffel relationship')\n",
    "\n",
    "plt.gray()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform GP regressions for each model : \n",
    "- for ODE model $\\dot{x}=f(x)$                 \n",
    "- for explicit geodesic model $\\ddot{x} + \\Gamma(x)\\dot{x}^{2} = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Gaussian Process Regression on ODE \n",
    "# GP regression (with pre-tuning of hyperparameters --> could be replaced by variance estimates computed from initial regression)\n",
    "training_iter = 20\n",
    "do_tuning = True\n",
    "print_tuning = False\n",
    "train_x = torch.from_numpy(u).float()\n",
    "train_y = torch.from_numpy(v).float()\n",
    "likelihood_GP = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "# list of ODE GP models\n",
    "modelGP_ODE_RBF = ExactGPModel('RBF', train_x, train_y, likelihood_GP)\n",
    "modelGP_ODE_linear = ExactGPModel('linear', train_x, train_y, likelihood_GP)\n",
    "modelGP_ODE_poly2 = ExactGPModel('polynomial', train_x, train_y, likelihood_GP, power=2)\n",
    "modelGP_ODE_poly3 = ExactGPModel('polynomial', train_x, train_y, likelihood_GP, power=3)\n",
    "modelGP_ODE_Matern = ExactGPModel('Matern', train_x, train_y, likelihood_GP, nu=2.5)\n",
    "\n",
    "# Find optimal model hyperparameters - akin to tuning\n",
    "if do_tuning:\n",
    "    for model_GP in [modelGP_ODE_RBF, modelGP_ODE_linear, modelGP_ODE_poly2, modelGP_ODE_poly3, modelGP_ODE_Matern]:\n",
    "        \n",
    "        model_GP.train()\n",
    "        likelihood_GP.train()\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': model_GP.parameters()},  # Includes GaussianLikelihood parameters\n",
    "        ], lr=0.1)\n",
    "\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_GP, model_GP) # \"Loss\" for GPs - the marginal log likelihood\n",
    "\n",
    "        for i in range(training_iter):\n",
    "\n",
    "            optimizer.zero_grad()         # Zero gradients from previous iteration\n",
    "            output = model_GP(train_x)       # Output from model\n",
    "            loss = -mll(output, train_y)  # Calc loss and backprop gradients\n",
    "            loss.backward()\n",
    "            if print_tuning:\n",
    "                print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                    i + 1, training_iter, loss.item(),\n",
    "                    model_GP.covar_module.base_kernel.lengthscale.item(),\n",
    "                    model_GP.likelihood.noise.item()\n",
    "                ))\n",
    "            optimizer.step()\n",
    "        \n",
    "        model_GP.eval()\n",
    "        likelihood_GP.eval()\n",
    "\n",
    "        \n",
    "# ------ Gaussian Process Regression on Christoffel coefficient\n",
    "\n",
    "train_x = torch.from_numpy(filtered_u).float()\n",
    "train_y = torch.from_numpy(filtered_ratio).float()\n",
    "\n",
    "# list of Hamiltonian GP models\n",
    "modelGP_HAM_RBF = ExactGPModel('RBF', train_x, train_y, likelihood_GP)\n",
    "modelGP_HAM_linear = ExactGPModel('linear', train_x, train_y, likelihood_GP)\n",
    "modelGP_HAM_poly2 = ExactGPModel('polynomial', train_x, train_y, likelihood_GP, power=2)\n",
    "modelGP_HAM_poly3 = ExactGPModel('polynomial', train_x, train_y, likelihood_GP, power=3)\n",
    "modelGP_HAM_Matern = ExactGPModel('Matern', train_x, train_y, likelihood_GP, nu=2.5)\n",
    "\n",
    "# Find optimal model hyperparameters - akin to tuning\n",
    "if do_tuning:\n",
    "    for model_GP in [modelGP_HAM_RBF, modelGP_HAM_linear, modelGP_HAM_poly2, modelGP_HAM_poly3, modelGP_HAM_Matern]:\n",
    "        \n",
    "        model_GP.train()\n",
    "        likelihood_GP.train()\n",
    "        optimizer = torch.optim.Adam([\n",
    "            {'params': model_GP.parameters()},  # Includes GaussianLikelihood parameters\n",
    "        ], lr=0.1)\n",
    "\n",
    "        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood_GP, model_GP) # \"Loss\" for GPs - the marginal log likelihood\n",
    "\n",
    "        for i in range(training_iter):\n",
    "\n",
    "            optimizer.zero_grad()         # Zero gradients from previous iteration\n",
    "            output = model_GP(train_x)       # Output from model\n",
    "            loss = -mll(output, train_y)  # Calc loss and backprop gradients\n",
    "            loss.backward()\n",
    "            if print_tuning:\n",
    "                print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                    i + 1, training_iter, loss.item(),\n",
    "                    model_GP.covar_module.base_kernel.lengthscale.item(),\n",
    "                    model_GP.likelihood.noise.item()\n",
    "                ))\n",
    "            optimizer.step()\n",
    "        \n",
    "        model_GP.eval()\n",
    "        likelihood_GP.eval()\n",
    "        \n",
    "        \n",
    "# ------ Plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "ax1.scatter(x=u, y=v, marker='o', color='k', s=10.)\n",
    "with torch.no_grad():\n",
    "    for model_GP in [modelGP_ODE_RBF, modelGP_ODE_linear, modelGP_ODE_poly2, modelGP_ODE_poly3, modelGP_ODE_Matern]:\n",
    "        u_line = np.linspace(u.min(), u.max(), 200)\n",
    "        f_preds = model_GP(torch.from_numpy(u_line).float())\n",
    "        f_mean = f_preds.mean\n",
    "        f_var = f_preds.variance\n",
    "        ax1.plot(u_line, gpu_numpy_detach(f_mean), label=model_GP.name)\n",
    "        lower, upper = f_mean - 2. * f_var, f_mean + 2. * f_var\n",
    "        ax1.fill_between(u_line, gpu_numpy_detach(lower), gpu_numpy_detach(upper), alpha=0.2)\n",
    "ax1.set_xlim(np.min(u) - 1e-4, np.max(u) + 1e-4)\n",
    "ax1.set_ylim(np.min(v) - 1e-4, np.max(v) + 1e-4)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('x_dot')\n",
    "ax1.set_title('GP regression on ODE function')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.scatter(x=filtered_u, y=filtered_ratio, marker='o', color='k', s=10.)\n",
    "with torch.no_grad():\n",
    "    for model_GP in [modelGP_HAM_RBF, modelGP_HAM_linear, modelGP_HAM_poly2, modelGP_HAM_poly3, modelGP_HAM_Matern]:\n",
    "        u_line = np.linspace(u.min(), u.max(), 200)\n",
    "        f_preds = model_GP(torch.from_numpy(u_line).float())\n",
    "        f_mean = f_preds.mean\n",
    "        f_var = f_preds.variance\n",
    "        ax2.plot(u_line, gpu_numpy_detach(f_mean), label=model_GP.name)\n",
    "        lower, upper = f_mean - 2. * f_var, f_mean + 2. * f_var\n",
    "        ax2.fill_between(u_line, gpu_numpy_detach(lower), gpu_numpy_detach(upper), alpha=0.2)\n",
    "ax2.set_xlim(np.min(filtered_u) - 1e-4, np.max(filtered_u) + 1e-4)\n",
    "ax2.set_ylim(np.min(filtered_ratio) - 1e-4, np.max(filtered_ratio) + 1e-4)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('- x_dotdot / x_dot^2')\n",
    "ax2.set_title('GP regression on Christoffel relationship')\n",
    "ax2.legend()\n",
    "\n",
    "plt.gray()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last steps for **ODE** study : \n",
    "- rconstruct characteristic trajectory $x_{ref}(t)$\n",
    "- define reference crossing time $\\tau_{ODE}$ and align others with it\n",
    "- compute statistics on $\\tau_{ODE}$ vs $x_{t_{REF}}$ vs $APOE$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Reference trajectory integration\n",
    "best_ODE_GP = modelGP_ODE_linear       # modelGP_ODE_RBF | modelGP_ODE_poly2\n",
    "\n",
    "nb_pts = 200\n",
    "t_obs_var = np.concatenate(t_real).ravel().var()\n",
    "t_line = np.linspace(np.concatenate(t_real).ravel().min() - .5*t_obs_var, np.concatenate(t_real).ravel().max() + .5*t_obs_var, nb_pts)\n",
    "x_line = np.linspace(np.concatenate(x_real).ravel().min(), np.concatenate(x_real).ravel().max(), nb_pts)\n",
    "t_ref = np.mean(t_line)\n",
    "pib_threshold =(1.2 - data_mean)/ data_std if standardize else 1.2\n",
    "i_start = np.argmin(np.abs(t_line-t_ref))\n",
    "x_init = np.array([pib_threshold])\n",
    "\n",
    "# slightly modifier ivp integrator \n",
    "def ivp_integrate_GP(model, t_eval, y0):\n",
    "    likelihood_GP = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    def modelGP_numpy_integrator(t, np_x):\n",
    "        x = torch.from_numpy(np_x).float()\n",
    "        x = x.view(1, np.size(np_x))    # batch size of 1\n",
    "        x.requires_grad = True\n",
    "        # dx = likelihood_GP(model(x)).mean\n",
    "        dx = model(x).mean\n",
    "        return gpu_numpy_detach(dx)\n",
    "    sequence = solve_ivp(fun=modelGP_numpy_integrator, t_span=(t_eval[0], t_eval[-1]), y0=y0, t_eval=t_eval, rtol=1e-10)\n",
    "    return sequence['y']\n",
    "\n",
    "forward_x = ivp_integrate_GP(model=best_ODE_GP, t_eval=t_line[i_start:], y0=x_init)\n",
    "backward_x = ivp_integrate_GP(model=best_ODE_GP, t_eval=t_line[:i_start+1][::-1], y0=x_init)\n",
    "reference_trajectory = np.concatenate([np.concatenate(backward_x).ravel()[::-1][:-1], np.concatenate(forward_x).ravel()])\n",
    "\n",
    "# ------ Alignment of trajectories via tau parameter | also include APOE information if available (from start, in additional column)\n",
    "labeler_func = lambda arr: int(np.sum(arr))\n",
    "y_lab = []\n",
    "initial_conditions = []\n",
    "initial_dtau = []\n",
    "time_values = []\n",
    "data_values = []\n",
    "rejected_values = []\n",
    "i_undone = 0\n",
    "i_total = 0\n",
    "for batch_idx, (positions, maskers, times, sequences, labels) in enumerate(all_loader):\n",
    "    for position, masker, time, sequence, label in zip(positions, maskers, times, sequences, labels):\n",
    "        # ------ REDO PROCESSING STEP IN THE SAME FASHION\n",
    "        t = gpu_numpy_detach(time[masker == True])\n",
    "        s = gpu_numpy_detach(sequence[masker == True])\n",
    "        n = len(t)\n",
    "        assert n > 2, \"At least 3 points required for a 2nd order derivative estimate\"\n",
    "        t_bar = np.mean(t)\n",
    "        \n",
    "        if which_regression == 'ridge':\n",
    "            # ------ Ridge Regression\n",
    "            # i) Fit Ridge regression\n",
    "            t_poly = polynomial_features.fit_transform(t.reshape(-1, 1))\n",
    "            ridge_regression.fit(t_poly, s)\n",
    "            theta = np.array(ridge_regression.coef_)\n",
    "            A_func = lambda t_ref: np.array([[1, t_ref, t_ref**2], [0., 1., 2*t_ref], [0., 0., 2]])\n",
    "            A_bar = A_func(t_bar)\n",
    "\n",
    "            # ii) Regress fitted data at mean time point (ie t_bar)\n",
    "            s_hat = A_bar.dot(theta)  \n",
    "        else:\n",
    "            # ------ Sigmoid Regression under isotonic constraint (c > 0)\n",
    "            # i) Fit logistic regression by OLS\n",
    "            def np_sigmoid(x, *p):\n",
    "                x0, y0, c = p\n",
    "                return c / (1 + np.exp(-(x-x0))) + y0\n",
    "            \n",
    "            def np_velocity_sigmoid(x, *p):\n",
    "                x0, y0, c = p\n",
    "                sigma = np_sigmoid(x, *p)\n",
    "                return 1. / c * (sigma - y0) * (c + y0 - sigma)\n",
    "            \n",
    "            def np_acceleration_sigmoid(x, *p):\n",
    "                x0, y0, c = p\n",
    "                sigma = np_sigmoid(x, *p)\n",
    "                return (1. / c) ** 2 * (sigma - y0) * (c + y0 - sigma) * (c + 2 * y0 - 2 * sigma)\n",
    "            \n",
    "            p_guess = np.array([np.median(t), np.median(s), 1.0])\n",
    "            eps = 1e-3\n",
    "            p_star, p_cov = curve_fit(f=np_sigmoid, xdata=t, ydata=s, p0=p_guess, bounds=([np.min(t), np.min(s), eps], [np.max(t), np.max(s), np.inf] ))\n",
    "            \n",
    "            s_0 = np_sigmoid(t_bar, *p_star)\n",
    "            s_1 = np_velocity_sigmoid(t_bar, *p_star)\n",
    "            s_2 = np_acceleration_sigmoid(t_bar, *p_star)\n",
    "\n",
    "            # ii) Regress fitted data at mean time point (ie t_bar) --> second derivative follows sigmoid relationship\n",
    "            s_hat = np.array([s_0, s_1, s_2])\n",
    "        \n",
    "        # ------ Registration of curves by time translation wrt observed point\n",
    "        i_relative = np.argmin(np.abs(reference_trajectory-s_hat[0]))\n",
    "        if 0 < i_relative < len(reference_trajectory):\n",
    "            tau_relative = t_line[i_relative] - t_bar\n",
    "            initial_dtau.append(tau_relative)\n",
    "            i_ic = np.argmin(np.abs(t_line - (t_ref + tau_relative)))\n",
    "            initial_conditions.append(reference_trajectory[i_ic])\n",
    "            time_values.append(t)\n",
    "            data_values.append(s)\n",
    "            y_lab.append(labeler_func(gpu_numpy_detach(label)))\n",
    "        else:\n",
    "            rejected_values.append(s_hat[0])\n",
    "            i_undone += 1\n",
    "        i_total += 1\n",
    "print('>> Rejected rate = {:.1f}% ({} / {})\\n'.format(100 * (i_undone / float(i_total)), i_undone, i_total))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 10))\n",
    "ax1.plot(destandardize_time(torch.from_numpy(t_line - t_ref)), destandardize_data(torch.from_numpy(reference_trajectory)), '-', c='r', linewidth=1., label='reference trajectory')\n",
    "ax1.axhline(y=1.2, label='reference threshold')\n",
    "for data, time, tau in zip(data_values, time_values, initial_dtau):\n",
    "    ax1.plot(destandardize_time(torch.from_numpy(time + tau - t_ref)), destandardize_data(torch.from_numpy(data)), '-.', linewidth=.8)\n",
    "ax1.set_xlabel('Age')\n",
    "ax1.set_ylabel('PiB')\n",
    "ax1.set_title('Aligned trajectories')\n",
    "\n",
    "idx_33 = np.argwhere(np.array(y_lab) == 6)\n",
    "idx_34 = np.argwhere(np.array(y_lab) == 7)\n",
    "idx_44 = np.argwhere(np.array(y_lab) == 8)\n",
    "ax2.scatter(x=destandardize_time(torch.from_numpy(np.array(initial_dtau)[idx_33])), y=np.array(initial_conditions)[idx_33], marker='o', color='r', s=20., label='APOE 33')\n",
    "ax2.scatter(x=destandardize_time(torch.from_numpy(np.array(initial_dtau)[idx_34])), y=np.array(initial_conditions)[idx_34], marker='x', color='b', s=20., label='APOE 34')\n",
    "ax2.scatter(x=destandardize_time(torch.from_numpy(np.array(initial_dtau)[idx_44])), y=np.array(initial_conditions)[idx_44], marker='h', color='g', s=20., label='APOE 44')\n",
    "ax2.axvline(x=destandardize_time(torch.from_numpy(np.array(t_ref))), label='reference time')\n",
    "ax2.set_xlabel('tau differential')\n",
    "ax2.set_ylabel('initial position')\n",
    "ax2.set_title('Initial conditions distribution at age {:d}'.format(int(destandardize_time(torch.from_numpy(np.array(t_ref))))))\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wilcoxon** statistical [test](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.wilcoxon.html) on APOE populations :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- randomly draw pairs between labels\n",
    "boxplot_stock = []\n",
    "boxplot_stock2 = []\n",
    "for ic, name in zip([idx_33, idx_34, idx_44], ['APOE 33', 'APOE 34', 'APOE 44']):\n",
    "    age_tref = int(gpu_numpy_detach(destandardize_time(torch.from_numpy(np.array([t_ref]))))[0])\n",
    "    age_pibpos = gpu_numpy_detach(destandardize_time(torch.from_numpy(t_ref - np.array(initial_dtau)[ic])))\n",
    "    pib_tref = gpu_numpy_detach(destandardize_data(torch.from_numpy(np.array(initial_conditions)[ic])))\n",
    "    boxplot_stock.append(age_pibpos)\n",
    "    boxplot_stock2.append(pib_tref)\n",
    "    print('{}'.format(name))\n",
    "    print('          age at PiB positive :     {:.2f} +- {:.2f}'.format(np.mean(age_pibpos), np.std(age_pibpos)))\n",
    "    print('          PiB at reference age {} : {:.2f} +- {:.2f}'.format(age_tref, np.mean(pib_tref), np.std(pib_tref)))\n",
    "\n",
    "# -------- compute wilcowon scores\n",
    "wilcox_age_01 = ranksums(x=np.array(initial_dtau)[idx_33].squeeze(), \n",
    "                         y=np.array(initial_dtau)[idx_34].squeeze())\n",
    "wilcox_age_02 = ranksums(x=np.array(initial_dtau)[idx_33].squeeze(), \n",
    "                         y=np.array(initial_dtau)[idx_44].squeeze())\n",
    "wilcox_age_12 = ranksums(x=np.array(initial_dtau)[idx_34].squeeze(), \n",
    "                         y=np.array(initial_dtau)[idx_44].squeeze())\n",
    "wilcox_pib_01 = ranksums(x=np.array(initial_conditions)[idx_33].squeeze(), \n",
    "                         y=np.array(initial_conditions)[idx_34].squeeze())\n",
    "wilcox_pib_02 = ranksums(x=np.array(initial_conditions)[idx_33].squeeze(), \n",
    "                         y=np.array(initial_conditions)[idx_44].squeeze())\n",
    "wilcox_pib_12 = ranksums(x=np.array(initial_conditions)[idx_34].squeeze(), \n",
    "                         y=np.array(initial_conditions)[idx_44].squeeze())\n",
    "\n",
    "print('\\n')\n",
    "print('Wilcoxon test p-value for difference in age at PIB positive between {} and {} = {:.3f}'.format('APOE 33', 'APOE 34', wilcox_age_01.pvalue))\n",
    "print('Wilcoxon test p-value for difference in PIB (at ref age) between {} and {} = {:.3f}'.format('APOE 33', 'APOE 34', wilcox_pib_01.pvalue))\n",
    "\n",
    "print('-----')\n",
    "print('Wilcoxon test p-value for difference in age at PIB positive between {} and {} = {:.3f}'.format('APOE 33', 'APOE 44', wilcox_age_02.pvalue))\n",
    "print('Wilcoxon test p-value for difference in PIB (at ref age) between {} and {} = {:.3f}'.format('APOE 33', 'APOE 44', wilcox_pib_02.pvalue))\n",
    "\n",
    "print('-----')\n",
    "print('Wilcoxon test p-value for difference in age at PIB positive between {} and {} = {:.3f}'.format('APOE 34', 'APOE 44', wilcox_age_12.pvalue))\n",
    "print('Wilcoxon test p-value for difference in PIB (at ref age) between {} and {} = {:.3f}'.format('APOE 34', 'APOE 44', wilcox_pib_12.pvalue))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "fig = plt.subplots(figsize=(10, 5))\n",
    "plt.boxplot(boxplot_stock)\n",
    "plt.xticks([i for i in range(1, 4)], ['APOE 33', 'APOE 34', 'APOE 44'], rotation=60)\n",
    "plt.ylabel('Age at PiB positive')\n",
    "plt.title('Age at PiB positive according to ODE regression')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.subplots(figsize=(10, 5))\n",
    "plt.boxplot(boxplot_stock2)\n",
    "plt.xticks([i for i in range(1, 4)], ['APOE 33', 'APOE 34', 'APOE 44'], rotation=60)\n",
    "plt.ylabel('PiB')\n",
    "plt.title('PiB at reference age {} y'.format(age_tref))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps for **Hamiltonian** study : \n",
    "- visualize inverse metric defined by GP regression by integrating  (up to multiplicative factor) \n",
    "- compute each trajectory $\\gamma_{i}$ by matching the best regression $\\gamma^{\\star}$ and computing its initial velocity $v^{\\star}_{i}(t)$\n",
    "- use initial velocities $v^{\\star}_{i}(t)$ as reference, study effect of varying reference time foe each individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Christoffel(nn.Module):\n",
    "\n",
    "    def __init__(self, fitted_model, xmin, xmax):\n",
    "        super(Christoffel, self).__init__()\n",
    "        self.model_GP = fitted_model\n",
    "        self.xref = xmin.clone().detach()\n",
    "        self.norm = 1.\n",
    "        self.xmin = xmin.view(-1, 1).clone().detach()\n",
    "        self.xmax = xmax.view(-1, 1).clone().detach()\n",
    "        self.normalization_call()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Christoffel symbol computation for given data\n",
    "        \"\"\"\n",
    "        return self.model_GP(data).mean\n",
    "    \n",
    "    def integrate(self, x, precision=100):\n",
    "        \"\"\"\n",
    "        Returns normalized inverse metric\n",
    "        \"\"\"\n",
    "        assert len(x.size()) == 2, \"must be of shape (n, 1)\"\n",
    "        x = x.repeat(1, precision)\n",
    "        n = len(x)\n",
    "        x0 = self.xref.repeat(n, precision)\n",
    "        line = torch.linspace(0., 1., precision).view(1, -1).repeat(n, 1)\n",
    "        x_evals = ((x-x0) * line + x0).float()\n",
    "        if not x_evals.requires_grad:\n",
    "            x_evals.requires_grad = True\n",
    "        evals = self.forward(x_evals.reshape(-1, 1)).reshape(n, precision)\n",
    "        trapezoidal_integration = 1./ (2 * precision) * torch.sum(evals[:, 1:]+evals[:, :-1], dim=-1)\n",
    "        integrated_x = torch.exp( - 2. * trapezoidal_integration).view(-1, 1)\n",
    "        rescaled_integrated_x = self.mref * integrated_x\n",
    "        return rescaled_integrated_x\n",
    "    \n",
    "    def normalization_call(self):\n",
    "        precision_1 = 100\n",
    "        precision_2 = 25\n",
    "        line_1 = (self.xmin + (self.xmax - self.xmin) * torch.linspace(0., 1., precision_1)).view(-1, 1).repeat(1, precision_2)\n",
    "        grid = (self.xref + (line_1 - self.xref) * torch.linspace(0., 1., precision_2).view(1, -1).repeat(precision_1, 1))\n",
    "        evals = self.forward(grid.reshape(-1, 1)).reshape(precision_1, precision_2)\n",
    "        trapezoidal_integration = 1./ (2 * precision_2) * torch.sum(evals[:, 1:]+evals[:, :-1], dim=-1)\n",
    "        integrated_out = torch.exp( - 2. * trapezoidal_integration)\n",
    "        integral = 1./ (2 * precision_1) * torch.sum(integrated_out[1:]+integrated_out[:-1])\n",
    "        self.mref = (self.norm / integral).view(-1, 1)\n",
    "    \n",
    "\n",
    "class Hamiltonian(object):\n",
    "    \"\"\"\n",
    "    Hamiltonian class allowing for autodiff computation of geodesics and field observation from Christoffel symbol\n",
    "    /!\\ Implementation restricted to dimension 1 and BATCH, not extensible to higher dim as is\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, christoffel):\n",
    "        super(Hamiltonian, self).__init__()\n",
    "        self.christoffel = christoffel\n",
    "        self.dimension = 1\n",
    "        self.J = torch.cat((torch.cat((torch.zeros(self.dimension, self.dimension),\n",
    "                                       torch.eye(self.dimension)), dim=1),\n",
    "                            torch.cat((torch.diag(torch.Tensor([-1] * self.dimension)),\n",
    "                                       torch.zeros(self.dimension, self.dimension)), dim=1)), dim=0)\n",
    "    \n",
    "    def inversemetric(self, data, precision=100):\n",
    "        return self.christoffel.integrate(data, precision)\n",
    "    \n",
    "    def metric(self, q, precision=100):\n",
    "        return 1. / self.inversemetric(q, precision)\n",
    "    \n",
    "    def velocity_to_momenta(self, q, v):\n",
    "        return self.metric(q) * v\n",
    "\n",
    "    def momenta_to_velocity(self, q, p):\n",
    "        return self.inversemetric(q) * p\n",
    "    \n",
    "    def dHdq(self, q, p):\n",
    "        return 2 * p**2 * self.inversemetric(q) * self.christoffel(q) .view(-1, 1)\n",
    "    \n",
    "    def dHdp(self, q, p):\n",
    "        return 2 * p * self.inversemetric(q)\n",
    "    \n",
    "    def hamiltonian(self, z):\n",
    "        \"\"\"\n",
    "        Hamiltonian from inverse metric computations : H(q,p) = .5 * <p|Qp>\n",
    "        \"\"\"\n",
    "        bts = z.size(0)\n",
    "        q, p = torch.split(z, split_size_or_sections=z.shape[1]//2, dim=1)\n",
    "        Q_batched = self.inversemetric(q)\n",
    "        Q_batched = Q_batched.unsqueeze(-1) if len(Q_batched.size()) < 3 else Q_batched\n",
    "        return .5 * torch.bmm(p.unsqueeze(1), torch.bmm(Q_batched, p.unsqueeze(-1))).squeeze(-1)\n",
    "    \n",
    "    def get_grad(self, z, create_graph=False, retain_graph=False, allow_unused=False):\n",
    "        bts, dim = z.size()\n",
    "        H = self.hamiltonian(z)\n",
    "        dH = torch.autograd.grad(outputs=H, inputs=z, grad_outputs=torch.ones((bts, 1)), \n",
    "                                 create_graph=create_graph, retain_graph=retain_graph, \n",
    "                                 allow_unused=allow_unused)[0]\n",
    "        return dH\n",
    "    \n",
    "    def get_Rgrad(self, z, create_graph=False, retain_graph=False):\n",
    "        \"\"\"\n",
    "        Returns rotated Hamiltonian, driving geodesic equations\n",
    "        \"\"\"\n",
    "        return self.get_grad(z, create_graph, retain_graph) @ self.J.t()\n",
    "    \n",
    "    def require_grad_field(self, status):\n",
    "        for e in self.christoffel.parameters():\n",
    "            e.requires_grad = status\n",
    "            \n",
    "            \n",
    "def torchdiffeq_torch_integrator(GP_model, t_eval, y0, method='rk4', adjoint_boolean=False, create_graph=True, retain_graph=True):\n",
    "    \n",
    "    assert str.upper(method) in ['EULER', 'MIDPOINT', 'RK4', 'DOPRI5']\n",
    "    \n",
    "    class dummy_integrator(nn.Module):\n",
    "        \n",
    "        def __init__(self, fitted_model, create_graph, retain_graph):\n",
    "            super(dummy_integrator, self).__init__()\n",
    "            self.hamiltonian_nn = Hamiltonian(Christoffel(fitted_model, xmin=torch.Tensor([1.]), xmax=torch.Tensor([2.])))\n",
    "            self.retain_graph = retain_graph\n",
    "            self.create_graph = create_graph\n",
    "        \n",
    "        def forward(self, t, x):\n",
    "            return self.hamiltonian_nn.get_Rgrad(x, create_graph=self.create_graph, retain_graph=self.retain_graph)\n",
    "\n",
    "    dummy_nn = dummy_integrator(GP_model, create_graph, retain_graph)\n",
    "    if adjoint_boolean:\n",
    "        z_out = torchdiffeq.odeint_adjoint(dummy_nn, y0, t_eval, method=str.lower(method))\n",
    "    else:\n",
    "        z_out = torchdiffeq.odeint(dummy_nn, y0, t_eval, method=str.lower(method))\n",
    "    return z_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Hamiltonian field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_HAM_GP = modelGP_HAM_linear     # modelGP_HAM_RBF |  modelGP_HAM_poly2\n",
    "\n",
    "# -------- Christoffel symbol estimate (trapezoidal integration)\n",
    "filtered_u_line = np.linspace(filtered_u.min(), filtered_u.max(), 500)\n",
    "ful_torch = totorch(filtered_u_line, 'float')\n",
    "Gamma = Christoffel(fitted_model=best_HAM_GP, xmin=ful_torch.min().clone().detach(), xmax=ful_torch.max().clone().detach())\n",
    "\n",
    "# Hamiltonian object derivation\n",
    "hamiltonian_fn = Hamiltonian(Gamma)\n",
    "inverse_estimate = hamiltonian_fn.inversemetric(ful_torch, precision=1000)\n",
    "metric_estimate = hamiltonian_fn.metric(ful_torch)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 18), sharex=True)\n",
    "ax1.plot(destandardize_data(torch.from_numpy(filtered_u_line)), np.squeeze(gpu_numpy_detach(inverse_estimate)), '-.r', label='inverse metric')\n",
    "ax1bis = ax1.twinx()\n",
    "ax1bis.plot(destandardize_data(torch.from_numpy(filtered_u_line)), np.squeeze(gpu_numpy_detach(metric_estimate)), '-.b', label='metric')\n",
    "ax1.set_xlabel('PiB index')\n",
    "ax1.set_ylabel('inverse metric')\n",
    "ax1bis.set_ylabel('metric')\n",
    "ax1.set_title('Estimated metric (normalized) for {}'.format(best_HAM_GP.name))\n",
    "ax1.legend(loc=0)\n",
    "ax1bis.legend(loc=2)\n",
    "\n",
    "# -------- associated Hamiltonian\n",
    "momentum_p = gpu_numpy_detach(hamiltonian_fn.velocity_to_momenta(totorch(filtered_u, 'float'), totorch(v[filtered_index], 'float')))\n",
    "ax2.scatter(x=destandardize_data(torch.from_numpy(filtered_u)), y=momentum_p, marker='o', color='r', s=40.)\n",
    "Y, X = np.mgrid[momentum_p.min():momentum_p.max():150j, \n",
    "                filtered_u.min():filtered_u.max():150j]\n",
    "_, X_std = np.mgrid[momentum_p.min():momentum_p.max():150j, \n",
    "                    gpu_numpy_detach(destandardize_data(torch.from_numpy(filtered_u))).min():gpu_numpy_detach(destandardize_data(torch.from_numpy(filtered_u))).max():150j]\n",
    "Z = torch.from_numpy(np.stack([X, Y])).reshape(2, len(X) * len(Y)).t().type(torch.float)\n",
    "Z.requires_grad = True\n",
    "Field_grad = hamiltonian_fn.get_Rgrad(Z, retain_graph=False, create_graph=False)\n",
    "Field_grid = Field_grad.reshape(len(X), len(Y), 2).permute(2, 0, 1)\n",
    "F_speed = gpu_numpy_detach(torch.norm(Field_grid, p=2, dim=0))\n",
    "Field_grid = gpu_numpy_detach(Field_grid)\n",
    "lw = 10 * F_speed / F_speed.max()\n",
    "\n",
    "#  Varying density along a streamline\n",
    "strm = ax2.streamplot(X_std, Y, Field_grid[0], Field_grid[1], linewidth=lw, cmap='plasma')\n",
    "ax2.set_title('Vector field')\n",
    "ax2.set_ylim(np.min(momentum_p) - 1e-4, np.max(momentum_p) + 1e-4)\n",
    "ax2.set_xlabel('PiB')\n",
    "ax2.set_ylabel('momenta')\n",
    "ax2.set_title('GP regression {} on ODE function'.format(best_HAM_GP.name))\n",
    "\n",
    "# Zooming on outliers excluded\n",
    "eps_filter = .05\n",
    "fIDX = np.where((momentum_p > np.quantile(momentum_p, eps_filter)) & (momentum_p < np.quantile(momentum_p, 1 - eps_filter)))[0]\n",
    "Y, X = np.mgrid[momentum_p[fIDX].min():momentum_p[fIDX].max():150j, filtered_u[fIDX].min():filtered_u[fIDX].max():150j]\n",
    "_, X_std = np.mgrid[momentum_p[fIDX].min():momentum_p[fIDX].max():150j, \n",
    "                    gpu_numpy_detach(destandardize_data(torch.from_numpy(filtered_u[fIDX]))).min():gpu_numpy_detach(destandardize_data(torch.from_numpy(filtered_u[fIDX]))).max():150j]\n",
    "Z = torch.from_numpy(np.stack([X, Y])).reshape(2, len(X) * len(Y)).t().type(torch.float)\n",
    "Z.requires_grad = True\n",
    "# Hamiltonian integral curves\n",
    "Field_grad = hamiltonian_fn.get_Rgrad(Z, retain_graph=False, create_graph=False)\n",
    "Field_grid = Field_grad.reshape(len(X), len(Y), 2).permute(2, 0, 1)\n",
    "F_speed = gpu_numpy_detach(torch.norm(Field_grid, p=2, dim=0))\n",
    "Field_grid = gpu_numpy_detach(Field_grid)\n",
    "lw = 10 * F_speed / F_speed.max()\n",
    "ax3.scatter(x=destandardize_data(torch.from_numpy(filtered_u[fIDX])), y=momentum_p[fIDX], marker='o', color='r', s=40.)\n",
    "strm_ = ax3.streamplot(X_std, Y, Field_grid[0], Field_grid[1], linewidth=lw, cmap='plasma')\n",
    "\n",
    "ax3.set_title('Vector field (zoom)')\n",
    "ax3.set_ylim(np.min(momentum_p[fIDX]) - 1e-4, np.max(momentum_p[fIDX]) + 1e-4)\n",
    "ax3.set_xlabel('PiB')\n",
    "ax3.set_ylabel('momenta')\n",
    "ax3.set_title('GP regression {} on ODE function (zoom)'.format(best_HAM_GP.name))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform **geodesic regression** to estimate initial conditions (gradient descent):\n",
    "- makes more sense to do it at reference time for everyone directly (instead of optimizing on data and then projecting elsewhere --> compare the perf)\n",
    "- initialization from Ridge regression estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pts = 200\n",
    "t_obs_var = np.concatenate(t_real).ravel().var()\n",
    "t_line = np.linspace(np.concatenate(t_real).ravel().min() - 7*t_obs_var, np.concatenate(t_real).ravel().max() + 7*t_obs_var, nb_pts)\n",
    "x_line = np.linspace(np.concatenate(x_real).ravel().min(), np.concatenate(x_real).ravel().max(), nb_pts)\n",
    "t_ref = np.mean(t_line)\n",
    "pib_threshold =(1.2 - data_mean)/ data_std if standardize else torch.Tensor([1.2])\n",
    "\n",
    "# slightly modifier ivp integrator \n",
    "labeler_func = lambda arr: int(np.sum(arr))\n",
    "y_lab_H = []\n",
    "initial_conditions = []\n",
    "tref_initial_conditions = []\n",
    "time_values = []\n",
    "data_values = []\n",
    "rejected_values = []\n",
    "i_undone = 0\n",
    "i_total = 0\n",
    "hamiltonian_fn.require_grad_field(True)\n",
    "\n",
    "\n",
    "for batch_idx, (positions, maskers, times, sequences, labels) in tqdm(enumerate(all_loader)):\n",
    "    for i, (position, masker, time, sequence, label) in enumerate(zip(positions, maskers, times, sequences, labels)):\n",
    "        # ------ get data, estimate roughly position | derivative for initialization of gradient descent at t_bar (resp. t_ref)\n",
    "        t = gpu_numpy_detach(time[masker == True])\n",
    "        s = gpu_numpy_detach(sequence[masker == True])\n",
    "        n = len(t)\n",
    "        assert n > 2, \"At least 3 points required for a 2nd order derivative estimate\"\n",
    "        t_bar = np.mean(t)\n",
    "        s_bar = np.mean(s)\n",
    "        do_reg_fit = True\n",
    "        if do_reg_fit:\n",
    "            polynomial_features = PolynomialFeatures(degree=2,  include_bias=True)\n",
    "            alpha_ridge = 1e-10 if standardize else 1e1\n",
    "            ridge_regression = Ridge(alpha=alpha_ridge, fit_intercept=False)\n",
    "            t_poly = polynomial_features.fit_transform(t.reshape(-1, 1))\n",
    "            ridge_regression.fit(t_poly, s)\n",
    "            theta = np.array(ridge_regression.coef_)\n",
    "            A_func = lambda t: np.array([[1, t, t**2], [0., 1., 2*t], [0., 0., 2]])\n",
    "            A_bar = A_func(t_bar)\n",
    "            sdot_bar = A_bar.dot(theta)[1]  # np.abs() for constraint initialization to be positive velocity\n",
    "            p_bar = gpu_numpy_detach(hamiltonian_fn.velocity_to_momenta(q=torch.from_numpy(np.array(A_bar.dot(theta)[0])).view(-1,1), v=torch.from_numpy(np.array(sdot_bar)).view(-1,1)))\n",
    "        else:\n",
    "            p_bar = 1.\n",
    "        # ------ Geodesic regression of initial constraints via Gradient Descent steps (differentiability of overall)\n",
    "        lr = 1e-1\n",
    "        min_lr = 1e-5\n",
    "        lr_decay = .9\n",
    "        patience = 5\n",
    "        initial_condition = torch.Tensor([s_bar, p_bar]).view(1, -1)     # (batch, 2*dim)\n",
    "        torch_t_eval = torch.from_numpy(t)\n",
    "        torch_y = torch.from_numpy(s)\n",
    "        torch_t_eval.requires_grad = False\n",
    "        torch_y.requires_grad = False\n",
    "        optimizer = Adam([initial_condition], lr=lr, amsgrad=True)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay, patience=patience, \n",
    "                                      min_lr=min_lr, verbose=False, threshold_mode='abs')\n",
    "\n",
    "        # Optimize\n",
    "        epochs = 100\n",
    "        loss_ = 0\n",
    "        initial_condition.requires_grad = True\n",
    "        regressed_flow = 0\n",
    "        best_loss_ = np.inf\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            # ----- Linear regression with pseudo inverse\n",
    "            regressed_flow = torchdiffeq_torch_integrator(GP_model=best_HAM_GP, t_eval=torch_t_eval, y0=initial_condition, \n",
    "                                                          method='midpoint', adjoint_boolean=False, create_graph=True, retain_graph=True)\n",
    "            delta = regressed_flow.squeeze()[:,0] - torch_y\n",
    "            loss_ = torch.mean(delta**2, dim=0, keepdim=False)\n",
    "            loss_.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            scheduler.step(loss_)\n",
    "            # Check NaN\n",
    "            if gpu_numpy_detach(torch.isnan(initial_condition).sum()):\n",
    "                # print(epoch, initial_condition)\n",
    "                print('Nan encountered : breaking loop at epoch {}'.format(epoch))\n",
    "                break\n",
    "            \n",
    "            # Retrieve best regression parameters\n",
    "            if gpu_numpy_detach(loss_) < best_loss_:\n",
    "                best_loss_ = gpu_numpy_detach(loss_)\n",
    "                best_init_ = gpu_numpy_detach(initial_condition)\n",
    "\n",
    "        # ------ From previous computation, estimate geodesic by **shooting** from previous initial conditions to real observations\n",
    "        t_evaluator = torch.from_numpy(np.array([t[0], t_ref]))\n",
    "        best_y0s = torch.from_numpy(best_init_)\n",
    "        best_y0s.requires_grad = True\n",
    "        tref_regressed_flow = torchdiffeq_torch_integrator(GP_model=best_HAM_GP, t_eval=t_evaluator, y0=best_y0s, \n",
    "                                                           method='midpoint', adjoint_boolean=False, create_graph=True, retain_graph=False)\n",
    "        tref_initial_conditions.append(gpu_numpy_detach(tref_regressed_flow[-1]))\n",
    "        initial_conditions.append(best_init_)\n",
    "        time_values.append(t)\n",
    "        data_values.append(s)\n",
    "        y_lab_H.append(labeler_func(gpu_numpy_detach(label)))\n",
    "        i_total += 1\n",
    "print('>> Rejected rate = {:.1f}% ({} / {})\\n'.format(100 * (i_undone / float(i_total)), i_undone, i_total))\n",
    "\n",
    "idxH_33 = np.argwhere(np.array(y_lab_H) == 6)\n",
    "idxH_34 = np.argwhere(np.array(y_lab_H) == 7)\n",
    "idxH_44 = np.argwhere(np.array(y_lab_H) == 8)\n",
    "\n",
    "np_ic = np.concatenate([ic.reshape(-1, 1) for ic in initial_conditions], axis=-1)\n",
    "np_ic_ref = np.concatenate([ic.reshape(-1, 1) for ic in tref_initial_conditions], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12))\n",
    "# Initial conditions regression at reference age\n",
    "ax1.scatter(x=destandardize_data(torch.from_numpy(np_ic_ref[0, idxH_33])), y=np_ic_ref[1, idxH_33], marker='o', color='r', s=20., label='APOE 33')\n",
    "ax1.scatter(x=destandardize_data(torch.from_numpy(np_ic_ref[0, idxH_34])), y=np_ic_ref[1, idxH_34], marker='x', color='b', s=20., label='APOE 34')\n",
    "ax1.scatter(x=destandardize_data(torch.from_numpy(np_ic_ref[0, idxH_44])), y=np_ic_ref[1, idxH_44], marker='d', color='g', s=20., label='APOE 44')\n",
    "ax1.set_xlabel('PiB')\n",
    "ax1.set_ylabel('momentum')\n",
    "ax1.set_yscale('symlog')\n",
    "ax1.set_title('Initial conditions distribution at age {:d}'.format(int(destandardize_time(torch.from_numpy(np.array(t_ref))))))\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "# corresponding random geodesics\n",
    "idx1, idx2, idx3 = np.random.choice(idxH_33.squeeze(), 2, replace=False), np.random.choice(idxH_34.squeeze(), 2, replace=False), np.random.choice(idxH_44.squeeze(), 2, replace=False)\n",
    "ymin, ymax = np.inf, - np.inf\n",
    "for idds, name, color in zip([idx1, idx2, idx3], ['APOE 33', 'APOE 34', 'APOE 44'], ['r', 'b', 'g']):\n",
    "    for i, idd in enumerate(idds):\n",
    "        idd = int(idd)\n",
    "        # real observed data\n",
    "        if i == 0:\n",
    "            ax2.plot(destandardize_time(torch.from_numpy(time_values[idd])), destandardize_data(torch.from_numpy(data_values[idd])), \n",
    "                     linestyle='-', marker='o', color=color, label=name)\n",
    "        else:\n",
    "            ax2.plot(destandardize_time(torch.from_numpy(time_values[idd])), destandardize_data(torch.from_numpy(data_values[idd])), \n",
    "                     linestyle='-', marker='o', color=color)\n",
    "        ax2.scatter(x=destandardize_time(torch.from_numpy(np.array([t_ref]))), y=destandardize_data(torch.from_numpy(np_ic_ref)[0, idd]), \n",
    "                    marker='*', s=75, color='k')\n",
    "        # Compute real geodesic onward and backward of reference age\n",
    "        t_backward = torch.linspace(t_ref, np.concatenate(t_real).ravel().min() - 1*t_obs_var, 50)\n",
    "        t_forward = torch.linspace(t_ref, np.concatenate(t_real).ravel().max() + 1*t_obs_var,  50)\n",
    "        t_all = np.concatenate((gpu_numpy_detach(t_backward)[::-1][:-1], gpu_numpy_detach(t_forward)))\n",
    "        best_y0s = torch.from_numpy(np_ic_ref[:, idd]).view(1, -1)\n",
    "        best_y0s.requires_grad = True\n",
    "        forward_geo = torchdiffeq_torch_integrator(GP_model=best_HAM_GP, t_eval=t_forward, y0=best_y0s, \n",
    "                                                   method='midpoint', adjoint_boolean=False, create_graph=False, retain_graph=False)\n",
    "        backward_geo = torchdiffeq_torch_integrator(GP_model=best_HAM_GP, t_eval=t_backward, y0=best_y0s, \n",
    "                                                    method='midpoint', adjoint_boolean=False, create_graph=False, retain_graph=False)\n",
    "        all_geo = np.concatenate((gpu_numpy_detach(backward_geo)[::-1][:-1,0,0], gpu_numpy_detach(forward_geo)[:,0,0]))\n",
    "        ax2.plot(destandardize_time(torch.from_numpy(t_all)), destandardize_data(torch.from_numpy(all_geo)), linestyle='-.', color=color)\n",
    "        ymin = min(ymin, gpu_numpy_detach(destandardize_data(torch.from_numpy(data_values[idd]))).min())\n",
    "        ymax = max(ymax, gpu_numpy_detach(destandardize_data(torch.from_numpy(data_values[idd]))).max())\n",
    "\n",
    "ax2.set_ylim(ymin - .1 * (y_max - y_min), ymax + .1 * (y_max - y_min))      \n",
    "ax2.set_xlabel('Age')\n",
    "ax2.set_ylabel('PiB')\n",
    "ax2.set_title('Geodesic reconstruction for random observations')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to previous plots, with this time the initial conditions estimated at reference age (via geodesic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- STATIC VERSION (at reference age with geodesic interpolation)\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "# initial conditions distirbutions\n",
    "ax.scatter(x=destandardize_data(torch.from_numpy(np_ic_ref[0, idxH_33])), y=np_ic_ref[1, idxH_33], marker='*', color='r', s=50., label='APOE 33')\n",
    "ax.scatter(x=destandardize_data(torch.from_numpy(np_ic_ref[0, idxH_34])), y=np_ic_ref[1, idxH_34], marker='*', color='b', s=50., label='APOE 34')\n",
    "ax.scatter(x=destandardize_data(torch.from_numpy(np_ic_ref[0, idxH_44])), y=np_ic_ref[1, idxH_44], marker='*', color='g', s=50., label='APOE 44')\n",
    "\n",
    "# Reconstructed trajectories | estimated age at onset (aka PiB positivity)\n",
    "pib_at_t_ref = []   # storing estimated age of crossing PiB positivity --> some individuals SHOULD NOT be registered, here we do NOT have isotonic property\n",
    "for idds, name, color in zip([idxH_33.squeeze(), idxH_34.squeeze(), idxH_44.squeeze()], ['APOE 33', 'APOE 34', 'APOE 44'], ['r', 'b', 'g']):\n",
    "    for i, idd in enumerate(idds):\n",
    "        idd = int(idd)\n",
    "        # Time stretch : intrapolation | extrapolation\n",
    "        t_obs_min, t_obs_max = time_values[idd][0], time_values[idd][-1]\n",
    "        t_backward = torch.linspace(t_ref, np.concatenate(t_real).ravel().min() - 2*t_obs_var, 50)\n",
    "        t_forward = torch.linspace(t_ref, np.concatenate(t_real).ravel().max() + 2*t_obs_var,  50)\n",
    "        t_all = np.concatenate((gpu_numpy_detach(t_backward)[::-1][:-1], gpu_numpy_detach(t_forward)))\n",
    "        intrapolation_indexes = (t_all > t_obs_min) & (t_all < t_obs_max)\n",
    "        \n",
    "        best_y0s = torch.from_numpy(np_ic_ref[:, idd]).view(1, -1)\n",
    "        best_y0s.requires_grad = True\n",
    "        forward_geo = torchdiffeq_torch_integrator(GP_model=best_HAM_GP, t_eval=t_forward, y0=best_y0s, \n",
    "                                                   method='midpoint', adjoint_boolean=False, create_graph=True, retain_graph=False)\n",
    "        backward_geo = torchdiffeq_torch_integrator(GP_model=best_HAM_GP, t_eval=t_backward, y0=best_y0s, \n",
    "                                                    method='midpoint', adjoint_boolean=False, create_graph=True, retain_graph=False)\n",
    "        all_geo = np.concatenate((gpu_numpy_detach(backward_geo)[::-1][:-1,0,:], gpu_numpy_detach(forward_geo)[:,0,:]), axis=0)\n",
    "        \n",
    "        # Plot trajectories in phase space | emphasis on observed time sequence\n",
    "        ax.plot(destandardize_data(torch.from_numpy(all_geo[:, 0])), all_geo[:, 1], linestyle='-.', linewidth=.5, color=color)\n",
    "        ax.plot(destandardize_data(torch.from_numpy(all_geo[intrapolation_indexes, 0])), all_geo[intrapolation_indexes, 1], linestyle='-', linewidth=2., color=color)\n",
    "        posidx = np.where(all_geo[:, 0] > gpu_numpy_detach(pib_threshold))[0]\n",
    "        pib_at_t_ref.append(t_all[posidx[0]] if len(posidx) else None)\n",
    "\n",
    "ax.set_xlabel('PiB')\n",
    "ax.set_xlim(0.5, 2.5)\n",
    "ax.set_ylabel('momentum')\n",
    "ax.set_yscale('symlog')\n",
    "ax.set_title('Phase space trajectories from reference age {:d}'.format(int(destandardize_time(torch.from_numpy(np.array(t_ref))))))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wilcoxon unpaired test on initial conditions (separate, or as pair) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- randomly draw pairs between labels\n",
    "boxplot_stock1 = []\n",
    "boxplot_stock2 = []\n",
    "for i_, name in zip([idxH_33, idxH_34, idxH_44], ['APOE 33', 'APOE 34', 'APOE 44']):\n",
    "    age_tref = int(gpu_numpy_detach(destandardize_time(torch.from_numpy(np.array([t_ref]))))[0])\n",
    "    pib_tref = gpu_numpy_detach(destandardize_data(torch.from_numpy(np_ic_ref[0, i_])))\n",
    "    acc_tref = gpu_numpy_detach(destandardize_data(torch.from_numpy(np_ic_ref[1, i_])))\n",
    "    # trueage_tref = int(gpu_numpy_detach(np.array([pib_at_t_ref[int(i)] for i in i_])))\n",
    "    boxplot_stock1.append(pib_tref)\n",
    "    # boxplot_stock2.append(trueage_tref)\n",
    "    print('{}'.format(name))\n",
    "    print('          PiB at reference age {} : {:.2f} +- {:.2f}'.format(age_tref, np.mean(pib_tref), np.std(pib_tref)))\n",
    "    print('          Acc at reference age {} : {:.2f} +- {:.2f}'.format(age_tref, np.mean(acc_tref), np.std(acc_tref)))\n",
    "\n",
    "# -------- compute wilcowon vectorial scores\n",
    "wilcox_A_01 = ranksums(x=np_ic[0, idx_33].squeeze(), \n",
    "                         y=np_ic[0, idx_34].squeeze())\n",
    "wilcox_A_02 = ranksums(x=np_ic[0, idx_33].squeeze(), \n",
    "                         y=np_ic[0, idx_44].squeeze())\n",
    "wilcox_A_12 = ranksums(x=np_ic[0, idx_34].squeeze(), \n",
    "                         y=np_ic[0, idx_44].squeeze())\n",
    "wilcox_B_01 = ranksums(x=np_ic[1, idx_33].squeeze(), \n",
    "                         y=np_ic[1, idx_34].squeeze())\n",
    "wilcox_B_02 = ranksums(x=np_ic[1, idx_33].squeeze(), \n",
    "                         y=np_ic[1, idx_44].squeeze())\n",
    "wilcox_B_12 = ranksums(x=np_ic[1, idx_34].squeeze(), \n",
    "                         y=np_ic[1, idx_44].squeeze())\n",
    "\n",
    "print('\\n')\n",
    "print('Wilcoxon test p-value for difference in PIB (at ref age) between {} and {} = {:.3f}'.format('APOE 33', 'APOE 34', wilcox_A_01.pvalue))\n",
    "print('Wilcoxon test p-value for difference in vel (at ref age) between {} and {} = {:.3f}'.format('APOE 33', 'APOE 34', wilcox_B_01.pvalue))\n",
    "print('-----')\n",
    "print('Wilcoxon test p-value for difference in age at PIB positive between {} and {} = {:.3f}'.format('APOE 33', 'APOE 44', wilcox_A_02.pvalue))\n",
    "print('Wilcoxon test p-value for difference in vel (at ref age) between {} and {} = {:.3f}'.format('APOE 33', 'APOE 44', wilcox_B_02.pvalue))\n",
    "print('-----')\n",
    "print('Wilcoxon test p-value for difference in age at PIB positive between {} and {} = {:.3f}'.format('APOE 34', 'APOE 44', wilcox_A_12.pvalue))\n",
    "print('Wilcoxon test p-value for difference in vel (at ref age) between {} and {} = {:.3f}'.format('APOE 34', 'APOE 44', wilcox_B_12.pvalue))\n",
    "print('\\n')\n",
    "\n",
    "fig = plt.subplots(figsize=(10, 5))\n",
    "plt.boxplot(boxplot_stock1)\n",
    "plt.xticks([i for i in range(1, 4)], ['APOE 33', 'APOE 34', 'APOE 44'], rotation=60)\n",
    "plt.ylabel('PiB')\n",
    "plt.title('PiB score at reference age : {} years'.format(age_tref))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
